{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"p1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 1: Decision trees and machine learning fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first section of this practice notebook walks through the steps we learned using a toy datasets, and the second uses a real dataset. This way you get to practice what you have learned twice. Don't worry if you can't finish the entire second part with the real data set, most questions give automated feedback so you should be able to continue on your own time after the workshop. This notebook is adapted to this workshop from our teaching material, so some questions have been removed and you might noticed that the question numbers are not always sequential. Have fun and let us know if you have any questions!\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Run this cells to initialize a couple of libraries we use for the automated feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to start\n",
    "from hashlib import sha1\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Decision trees with a toy dataset \n",
    "<hr>\n",
    "\n",
    "Suppose you have three different job offers with comparable salaries and job descriptions. You want to decide which one to accept, and you want to make this decision based on which job is likely to make you happy. Being a very systematic person, you come up with three features associated with the offers, which are important for your happiness: whether the colleagues are supportive, whether there is work-hour flexibility, and whether the company is a start-up or not. So the `X` of your offer data looks as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "offer_data = {\n",
    "    # Features\n",
    "    \"supportive_colleagues\": [1, 0, 0, 1],\n",
    "    \"work_hour_flexibility\": [0, 0, 1, 1],\n",
    "    \"start_up\": [0, 1, 1, 1],    \n",
    "}\n",
    "\n",
    "offer_df = pd.DataFrame(offer_data)\n",
    "offer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to get predictions for these rows. In other words, for each row, you want to predict whether that job would make you **happy** or **unhappy**.   \n",
    "\n",
    "So you ask the following questions to some of your friends (who you think have similar notions of happiness) regarding their jobs:\n",
    "\n",
    "1. Do you have supportive colleagues? (1 for 'yes' and 0 for 'no')\n",
    "2. Do you have flexible work hours? (1 for 'yes' and 0 for 'no')\n",
    "3. Do you work for a start-up? (1 for 'start up' and 0 for 'non start up')\n",
    "4. Are you happy in your job? (happy or unhappy)\n",
    "\n",
    "Suppose you get the following data from this toy survey. You decide to train a machine learning model using this toy survey data and use this model to predict which job from `offer_df` is likely to make you happy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "happiness_data = {\n",
    "    # Features\n",
    "    \"supportive_colleagues\": [1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
    "    \"work_hour_flexibility\": [1, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "    \"start_up\": [1, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "    # Target\n",
    "    \"target\": [\n",
    "        \"happy\",\n",
    "        \"happy\",\n",
    "        \"happy\",\n",
    "        \"unhappy\",\n",
    "        \"unhappy\",\n",
    "        \"happy\",\n",
    "        \"happy\",\n",
    "        \"unhappy\",\n",
    "        \"unhappy\",\n",
    "        \"unhappy\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "train_df = pd.DataFrame(happiness_data)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Decision stump by hand \n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "With this toy dataset, build a decision stump (decision tree with only 1 split) by hand, splitting on the condition `supportive_colleagues <= 0.5`. What training accuracy would you get with this decision stump? Save the accuracy as a decimal in an object named `supportive_colleagues_acc`. \n",
    "\n",
    "> You do not have to show any calculations or code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "supportive_colleagues_acc = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Separating features and target\n",
    "rubric={autograde:2}\n",
    "\n",
    "Recall that in `scikit-learn`, before building a classifier, we need to separate features and target. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Separate features and target from `train_df` and save them in `X_train_toy` and `y_train_toy`, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_toy = None\n",
    "y_train_toy = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create a decision tree classifier object\n",
    "rubric={autograde:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a `DecisionTreeClassifier` object with `random_state=16` and store it in a variable called `toy_tree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the decision tree classifier\n",
    "...\n",
    "\n",
    "toy_tree = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 `fit` the decision tree classifier \n",
    "rubric={autograde:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Now train a decision tree model by calling `fit` on `toy_tree` with `X_train_toy` and `y_train_toy` created above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Visualize the trained decision tree\n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Visualize the trained decision tree model using the same function we used in the lecture notes (we have copied it here for you). Save the visualization tree returned by the function below in a variable called `toy_tree_viz`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "def display_tree(model, filled=True, impurity=False, ax=None, figsize=(12, 8), **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.subplots(figsize=figsize)[1]\n",
    "    return plot_tree(\n",
    "        model,\n",
    "        feature_names=model.feature_names_in_,\n",
    "        class_names=model.classes_.astype(str) if hasattr(model, 'classes_') else None, # To avoid errors when using regression trees\n",
    "        filled=filled,\n",
    "        impurity=impurity,\n",
    "        ax=ax,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_tree_viz = None\n",
    "\n",
    "...\n",
    "\n",
    "toy_tree_viz;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Depth of the tree\n",
    "rubric={autograde:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. What's the depth of the learned decision tree model? Save it as an integer in the variable `toy_depth` below. Hint: You can either input the depth manually by looking at your visualzation above or use the `.get_depth()` method of the decision tree object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.6\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "toy_depth = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Accuracy calculation\n",
    "rubric={autograde:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Evaluate the `toy_tree` on the training data (i.e., call `score()` on `X_train_toy` and `y_train_toy`) and store the score in a variable called `train_acc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.7\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_acc = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.8 Discussion\n",
    "rubric={reasoning:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Do you get perfect training accuracy? Why or why not? \n",
    "\n",
    "<details><summary>Solutions</summary>\n",
    "\n",
    "We do not get perfect training accuracy. Notice that the model made an \"error\" on example with index 8; the original target is \"unhappy\" and the predicted one is \"happy\". This is because we have some inconsistency in the training data; we have two examples in the dataset with exactly the same feature vectors but different targets.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.8\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Predicting on the offer data \n",
    "rubric={autograde:3}\n",
    "\n",
    "Recall that our goal is to predict in which jobs you are likely to be happy. The `offer_df` dataframe below has all the job offers you have received. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Using the trained decision tree above, predict the targets for all examples in `offer_df` and store them in the `predictions` variable below. In which of the job offers is the model predicting that you will be happy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "offer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.9\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Decision trees on Spotify Song Attributes dataset <a name=\"2\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introducing the dataset\n",
    "  \n",
    "For the rest of this practice session lab you'll be using [a dataset of Spotify Song Attributes](https://raw.githubusercontent.com/UBC-MDS/intro-to-ml-workshop/main/practice/p1/data/spotify.csv). The dataset contains a number of features of songs from 2017 and a binary variable `target` that represents whether the user liked the song (encoded as 1) or not (encoded as 0). See the documentation of all the features [here](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data splitting \n",
    "rubric={autograde:2}\n",
    "\n",
    "We have provided the code to read in the data CSV directly from the URL and to store it as a pandas dataframe named `spotify_df`.\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the dataframe into `train_df` and `test_df` with `random_state=123` and `test_size=0.2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f3f14b59fd7e6b8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To simplify the problem, we are only keeping a subset of the original columns\n",
    "url = 'https://raw.githubusercontent.com/UBC-MDS/intro-to-ml-workshop/main/practice/p1/data/spotify.csv'\n",
    "spotify_df = pd.read_csv(url, index_col=0)[['acousticness', 'danceability', 'liveness', 'tempo', 'energy', 'valence', 'loudness', 'target']]\n",
    "spotify_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the train test split funciton\n",
    "...\n",
    "\n",
    "train_df, test_df = ..., ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4d478b6cdc9bf88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exploratory data analysis (EDA)\n",
    "rubric={autograde:2}\n",
    "\n",
    "Right after splitting our data into train and test, we want to do some exploratory data analysis on the training dataframe. This analysis can help us idenitfy any data cleaning we need to do, what features could be informative for the target value, which models might be suirable for our problem, and more. The golden rule applies here to, the information we gather from the EDA will inform our down stream data science decision, so we don't want any information from the test data set to be used here.\n",
    "\n",
    "Since we didn't cover EDA in this workshop, we will show you a couple of summary statistics and plots that can be useful. For more inspiration of what you should look at during EDA, please [see some example here](https://joelostblom.github.io/altair_ally/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show summary statistics for each column\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b33320bcf667584a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The starter code below produces distribution plots and pairwise scatter plots for all the features, separated for positive (target=1, i.e., user liked the song) and negative (target=0, i.e., user disliked the song) examples. The histogram shows that extremely quiet songs tend to be disliked (more blue bars than orange on the left) and very loud songs also tend to be disliked (more blue than orange on the far right).\n",
    "\n",
    "Let's say that, for a particular feature, the distribution plots of that feature are identical for the two target classes. Does that mean the feature is not useful for predicting the target class?. \n",
    "No, the feature might still be useful, because it may be predictive in conjunction with other features. For example, the valence feature density plots (below) do indeed look quite overlapping. But it may be the case that very high valence in conjunction with low tempo is very predictive of a liked song. This type of pattern would not emerge in these individual density plots and not in the pairwise scatter plots if the relationship is complex, but a decision tree could potentially still learn it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.pairplot(train_df, hue='target', corner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "<hr>\n",
    "\n",
    "Now that we did some preliminary exploratory data analysis (EDA), let's move on to modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-706403e72adade4b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 3.1 Creating `X` and `y`\n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Separate `X` and `y` from `train_df` and `test_df` from the previous exercise and store them as `X_train`, `y_train`, `X_test`, `y_test`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The baseline model: `DummyClassifier`\n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Carry out 10-fold cross-validation on `DummyClassifier` object above using `cross_validate` on `X_train` and `y_train`. Pass `return_train_score=True` to `cross_validate`. Return the train score and store the object as a dataframe in the `dummy_score` variable below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the dummy classifier class and the cross_validate function\n",
    "...\n",
    "\n",
    "dummy_score = None\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-706403e72adade4b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 3.3\n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a `DecisionTreeClassifier` with `random_state=123` and store it in a variable called `spotify_tree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make the necessary import\n",
    "...\n",
    "\n",
    "spotify_tree = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cross-validation with `DecisionTreeClassifier`\n",
    "rubric={autograde:4}\n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "1. Carry out 10-fold cross validation with the `spotify_tree` object above using `cross_validate` on `X_train` and `y_train`. Pass `return_train_score=True` to `cross_validate`. Save the results as a pandas dataframe in a variable called `dt_scores_df`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt_scores_df = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters <a name=\"4\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4150979c1845a18c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1 Train and cross-validation plots\n",
    "rubric={autograde:12}\n",
    "\n",
    "In this exercise, you'll experiment with the `max_depth` hyperparameter of the decision tree classifier. See the [`DecisionTreeClassifier` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for more details.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore the `max_depth` hyperparameter. Run 10-fold cross-validation for trees with the following values of `max_depth`: `np.arange(1, 25, 2)`. Set the `random_state` of `DecisionTreeClassifier` to 123 in each case for reproducibility. \n",
    "2. For each `max_depth`, get both the mean train accuracy and the mean cross-validation accuracy.\n",
    "3. Make a plot with `max_depth` on the *x*-axis and the train and cross-validation accuracies on the *y*-axis. That is, your plot should have two curves, one for train and one for cross-validation. Include a legend to specify which is which and make sure each curve and the axes have the reasonable name. Save the plot to `max_depth_plot`.\n",
    "\n",
    "\n",
    "**There are some automatic checks on this question, but they don't represent the only way of going about solving this question, so you can check in with us that your chart looks reasonable if you think it is correct but the tests are failling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depths = np.arange(1, 25, 2)\n",
    "depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# max_depth_plot: the figure plotted for this exercise\n",
    "max_depth_plot = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Picking the best value for `max_depth`\n",
    "rubric={autograde:2}\n",
    "\n",
    "Before continuing, think about how changing the `max_depth` hyperparameter affects the training and cross-validation accuracy. \n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "    \n",
    "In case of the training data, a higher value of `max_depth` parameter results in higher accuracy. When the accuracy is 1.0, it means that the model is able to classify all training examples perfectly. This happens because for higher `max_depth` values, the decision tree learns a specific rule for almost all examples in the training data. In case of the cross-validation scores, initially the accuracy increases a bit and then it goes back down since the model is no longer learning the general relationship between the input features and the target, but instead learning about noise in the training data.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "1. From your results, pick the \"best\" `max_depth`, the one which gives the maximum cross-validation score. Store it in a variable called `best_max_depth` as an integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.3   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_max_depth = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Final assessment on the test split \n",
    "rubric={autograde:2}\n",
    "\n",
    "Now that we have our finalized model, we are ready to evaluate it on the test set!\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a decision tree model `best_model` using the `best_max_depth` you chose in the previous exercise. \n",
    "2. Fit the `best_model` on the _entire training set_ (`X_train` and `y_train`). \n",
    "2. Compute the test score (on `X_test` and `y_test`) and store it in a variable called `test_score` below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "test_score = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Visualizing Spotify decision tree\n",
    "rubric={autograde:3}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Visualize `best_model` with the `display_tree` function from Exercise 1.5 with `counts=True`. Store the visualization in `spotify_tree_viz` variable below. \n",
    "2. Which feature did the model pick as the best feature? In other words, what feature did the model use for the first split? Store the name of the feature as a string in the variable called `best_feat` below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spotify_tree_viz = None\n",
    "best_feat = None\n",
    "\n",
    "...\n",
    "\n",
    "spotify_tree_viz;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.6\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Analysis\n",
    "rubric={reasoning:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Reflect on the following questions:\n",
    "\n",
    "1. How do the test scores compare to the cross-validation scores? Briefly discuss. \n",
    "2. Why can't you simply pick the value of `max_depth` that gives the best accuracy on the training data? (Answer in maximum 2 to 3 sentences.)\n",
    "3. Do you think that the `max_depth` you chose would generalize to other \"spotify\" datasets (i.e., data on other spotify users)?\n",
    "\n",
    "\n",
    "<details><summary>Solutions</summary>\n",
    "    \n",
    "1. We see the test score is a bit higher compared to the cross-validation score. But I would not trust this result too much. Looking at the plot, we can see the cv score plot is quite \"bumpy\" and even if `max_depth=5` is a pretty good value, there is probably also some luck involved there.\n",
    "2. If we are to pick `max_depth` simply based on the training data, it'll pick the lowest value for the parameter as it performs best on the training set. (See the table and plot in 5.1.) That said, that model would be overfit and it won't generalize well on the validation data. That's why we treat it as a hyperparameter and pick the best value based on the cross-validation accuracy. \n",
    "3. Whether the chosen `max_depth` generalizes to other users or not would depend upon how similar the new user is to this user. In other words, whether the training data for this user is representative of the new user or not. That said, the chosen `max_depth` of 5 would most like do better than if we had chosen a higher depth.  \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Congratulations on practice 1! Well done ðŸ‘ðŸ‘! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not supportive_colleagues_acc is None, \"Are you setting the provided variable?\"\n>>> assert (\n...     sha1(str(supportive_colleagues_acc).encode(\"utf8\")).hexdigest() == \"1469842b4307d36cccb487dc989f21016daadbcc\"), \"Your answer is incorrect, see traceback above.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not X_train_toy is None, \"Are you using the correct variable?\"\n>>> assert not y_train_toy is None, \"Are you using the correct variable?\"\n>>> assert X_train_toy.shape == (10, 3), \"X_train_toy shape is incorrect\"\n>>> assert y_train_toy.shape == (10,), \"y_train_toy shape is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3": {
     "name": "q1.3",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(toy_tree, DecisionTreeClassifier), \"DecisionTreeClassifier was not created properly\"\n>>> assert (toy_tree.get_params().get(\"random_state\") == 16), \"Please set the random state to 16\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.4": {
     "name": "q1.4",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert toy_tree.get_depth() in range(2, 4, 1), \"DecisionTreeClassifier was not fitted properly\"\n>>> assert toy_tree.get_n_leaves() in range(3, 5, 1), \"DecisionTreeClassifier was not fitted properly\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.5": {
     "name": "q1.5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not toy_tree_viz is None, \"Are you using the provided variable?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.6": {
     "name": "q1.6",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not toy_depth is None, \"Are you using the provided variable?\"\n>>> assert (sha1(str(toy_depth).encode(\"utf-8\")).hexdigest() == \"77de68daecd823babbb58edb1c8e14d7106e83bb\"), \"The depth is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.7": {
     "name": "q1.7",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not train_acc is None, \"Are you using the provided train_acc variable?\"\n>>> assert (sha1(str(np.round(train_acc, 2)).encode(\"utf-8\")).hexdigest() == \"1469842b4307d36cccb487dc989f21016daadbcc\"), \"The score is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.9": {
     "name": "q1.9",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not predictions is None, \"Are you storing predictions in the provided variable predictions?\"\n>>> assert (predictions == toy_tree.predict(offer_df)).all(), \"Your predictions do not look as expected.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": [
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert (not train_df is None and not test_df is None), \"Are you using the provided variables?\"\n>>> n_total_samples = spotify_df.shape[0]\n>>> assert test_df.shape[0] == round(n_total_samples * 0.2) + (n_total_samples % 5 > 0), \"Are you using the provided test size?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(train_df.iloc[30][\"liveness\"], 0.268), \"Are you using the provided random state?\"\n>>> assert np.isclose(test_df.iloc[88][\"danceability\"], 0.727), \"Are you using the provided random state?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(X_train, pd.DataFrame), \"X_train is not created correctly\"\n>>> assert isinstance(y_train, pd.Series), \"y_train is not created correctly\"\n>>> assert isinstance(X_test, pd.DataFrame), \"X_test is not created correctly\"\n>>> assert isinstance(y_test, pd.Series), \"y_test is not created correctly\"\n>>> assert X_train.shape == (1613, 7), \"X_train has the wrong shape\"\n>>> assert X_test.shape == (404, 7), \"X_test has the wrong shape\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not dummy_scores is None, \"Are you using the provided variable?\"\n>>> assert(dummy_scores.shape[0] == 10)\n>>> \n>>> dummy2 = DummyClassifier()\n>>> dummy_scores2 = pd.DataFrame(cross_validate(dummy2, X_train, y_train, cv=10, return_train_score=True))\n>>> assert(dummy_scores['test_score'].mean() == dummy_scores2['test_score'].mean())\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.3": {
     "name": "q3.3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(spotify_tree, DecisionTreeClassifier), \"DecisionTreeClassifier was not created properly\"\n>>> assert (spotify_tree.get_params().get(\"random_state\") == 123), \"Please set the random state to 123\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.4": {
     "name": "q3.4",
     "points": [
      1,
      1,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(dt_scores_df, pd.DataFrame), \"dt_scores_df should be a DataFrame\"\n>>> assert len(dt_scores_df) == 10, \"Please check parameters of `cross_validate`\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert dt_scores_df.shape[0] == 10, \"Are you carrying out 10-fold cross-validation?\"\n>>> assert dt_scores_df.shape[1] == 4, \"Are you passing return_train_scores = True?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(round(dt_scores_df[\"test_score\"].mean(), 3), 0.634), \"Your test scores are incorrect\"\n>>> assert np.isclose(round(dt_scores_df[\"train_score\"].mean(), 3), 0.999), \"Your train scores are incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.1": {
     "name": "q4.1",
     "points": [
      1,
      2,
      2,
      2,
      5
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert X_train.shape == (1613, 7), \"Shape is incorrect\"\n>>> assert y_train.shape == (1613,), \"Shape is incorrect\"\n>>> assert X_test.shape == (404, 7), \"Shape is incorrect\"\n>>> assert y_test.shape == (404,), \"Shape is incorrect\"\n>>> \n>>> assert (round(X_train.iloc[123][\"loudness\"], 2) == -10.10), \"Are you using the correct X_train, y_train, X_test, y_test?\"\n>>> assert (round(y_train.iloc[62], 2) == 0.00), \"Are you using the correct X_train, y_train, X_test, y_test?\"\n>>> assert (round(X_test.iloc[234][\"valence\"], 2) == 0.18), \"Are you using the correct X_train, y_train, X_test, y_test?\"\n>>> assert (round(y_test.iloc[399], 2) == 1.00), \"Are you using the correct X_train, y_train, X_test, y_test?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not max_depth_plot is None, \"Are you storing the plot in a variable?\"\n>>> assert (len(max_depth_plot.lines) == 2), \"Please plot both the train accuracy and the cross-validation accuracy\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Check if the expected max_depths are on the x-axis\n>>> assert \"depth\" in max_depth_plot.get_xlabel().lower(), \"Please give x-axis a reasonable name\"\n>>> assert \"accuracy\" in max_depth_plot.get_ylabel().lower(), \"Please give y-axis a reasonable name\"\n>>> assert (max_depth_plot.lines[0].get_xdata() == np.arange(1, 25, 2)).all(), \"Please use the provided max_depth values\"\n>>> assert len(max_depth_plot.lines[1].get_xdata()) == len(np.arange(1, 25, 2)), \"Please use the provided max_depth values\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Check if the curves are labeled with reasonable names\n>>> assert any(label in max_depth_plot.legend().get_texts()[0].get_text().lower() for label in [\"train\", \"test\", \"cv\"]), \"Please label your curves with reasonable names\"\n>>> assert any(label in max_depth_plot.legend().get_texts()[1].get_text().lower().lower() for label in [\"train\", \"test\", \"cv\"]), \"Please label your curves with reasonable names\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Get train and test data points\n>>> legend_texts = max_depth_plot.legend().get_texts()\n>>> if \"train\" in legend_texts[0].get_text():\n...     train_xydata = max_depth_plot.lines[0].get_xydata()\n...     valid_xydata = max_depth_plot.lines[1].get_xydata()\n... else:\n...     train_xydata = max_depth_plot.lines[1].get_xydata()\n...     valid_xydata = max_depth_plot.lines[0].get_xydata()\n>>> \n>>> # Training scores should increase when max_depth increases\n>>> assert np.isclose(round(train_xydata[2][1], 3), 0.728, atol=0.02), \"The training data points are incorrect\"\n>>> assert (train_xydata[6][1] >= train_xydata[2][1]), \"The training data points are incorrect\"\n>>> assert (train_xydata[10][1] >= train_xydata[6][1]), \"The training data points are incorrect\"\n>>> \n>>> # Test scores\n>>> assert np.isclose(round(valid_xydata[2][1], 3), 0.654, atol=0.02), \"The training data points are incorrect\"\n>>> assert np.isclose(round(valid_xydata[6][1], 3), 0.632, atol=0.02), \"The training data points are incorrect\"\n>>> assert np.isclose(round(valid_xydata[10][1], 3), 0.634, atol=0.02), \"The training data points are incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.3": {
     "name": "q4.3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not best_max_depth is None, \"Are you using the provided variable?\"\n>>> assert sha1(str(best_max_depth).encode('utf-8')).hexdigest() == 'ac3478d69a3c81fa62e60f5c3696165a4e5e6ac4', \"Are you picking the best_max_depth which gives the highest cross-validation score?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.4": {
     "name": "q4.4",
     "points": [
      2,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not best_model is None, \"Are you creating a tree called best_spotify_tree?\"\n>>> assert best_model.get_params()['random_state'] == 123, \"Are you setting the random_state to 123?\"\n>>> assert best_model.get_n_leaves() in range(25, 30), \"Are you fitting best_spotify_tree?\"\n>>> assert best_model.get_depth() == best_max_depth, \"Are you fitting best_spotify_tree?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert (round(test_score, 3) == 0.661), \"Your test score seems off. Are you training on the entire training data?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.5": {
     "name": "q4.5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not best_feat is None, \"Are you setting the best_feat variable?\"\n>>> assert sha1(best_feat.encode('utf-8')).hexdigest() == '6cd560e4ccffc46287399c5f226bb2cc95e8c987', \"The best_feat seems incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
