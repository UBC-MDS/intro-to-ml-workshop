{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"p2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2: Preprocessing with `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3247a4b883a670c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "A crucial step when using machine learning algorithms on real-world datasets is preprocessing. This assignment will give you some practice of data preprocessing and building a supervised machine learning pipeline on a medium-sized dataset which has different types of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Dataset and preliminary EDA\n",
    "<hr>\n",
    "\n",
    "You will be working with [the adult census dataset](https://www.kaggle.com/uciml/adult-census-income#). We've already put the data CSV under the data folder. \n",
    "\n",
    "This is a classification dataset and the classification task is to predict whether income exceeds 50K per year or not based on the census data. Go through the information on [the dataset and features](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "The starter code below loads the data CSV (assuming that it is saved as `adult.csv` under the data folder). \n",
    "\n",
    "_Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "census_df = pd.read_csv(\"data/adult.csv\")\n",
    "census_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data splitting \n",
    "rubric={autograde}\n",
    "\n",
    "In order to avoid violation of the golden rule, the first step before we do anything is splitting the data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into `train_df` (40%) and `test_df` (60%) with `random_state = 123`. Keep the target column (`income`) in the splits so that we can use it in the exploratory data analysis.  \n",
    "\n",
    "_Usually, having more data for training is a good idea. But here I'm using 40%/60% split because running cross-validation with this dataset can take a long time on a modest laptop. A smaller training data means it will be a bit faster to train the model on your laptop. A side advantage of this is that with a bigger test split, we'll have a more reliable estimate of the model performance!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = None\n",
    "test_df = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our `train_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some missing values represented with a \"?\". Probably these were the questions not answered by some people during the census.  Usually `.describe()` or `.info()` methods would give you information on missing values. But here, they won't pick \"?\" as missing values because they are encoded as strings instead of an actual NaN in Python. So let's replace them with `np.nan` before we carry out EDA. If you do not do it, you'll encounter an error later on when you try to pass this data to a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.replace(\"?\", np.nan)\n",
    "test_df = test_df.replace(\"?\", np.nan)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"?\" symbols are now replaced with NaN values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 `describe()` method\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Examine the output of `train_df.describe()` with `include='all'` argument and store it in a variable called `census_summary`.\n",
    "2. What are the highest hours per week someone reported? Store it in a variable called `max_hours_per_week`.\n",
    "3. What is the most frequently occurring occupation in this dataset? Store it in a variable called `most_freq_occupation`.\n",
    "4. Store the column names of the columns with missing values as a list in a variable called `missing_vals_cols`. \n",
    "5. Store the column names of all numeric-looking columns, irrespective of whether you want to include them in your model or not, as a list in a variable called `numeric_cols`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "census_summary = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_hours_per_week = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_freq_occupation = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_vals_cols = None\n",
    "numeric_cols = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sorting the lists for the autograder\n",
    "missing_vals_cols.sort()\n",
    "numeric_cols.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (Optional) 1.3 Visualizing features\n",
    "rubric={point}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. For each numeric feature in `numeric_cols` you identified above, visualize the histograms for <=50K and >50K classes. \n",
    "2. Write a sentence or two describing your observations. \n",
    "\n",
    "> You can use the library of your choice for visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2: Identifying different feature types and transformations  \n",
    "<hr>\n",
    "\n",
    "Usually, data comes in a format which is not directly passed to machine learning models. A machine learning practitioner needs to examine each column carefully and come up with a way to effectively encode the information given in each column. Let's identify what kind of features do we have and consider some reasonable ways to encode them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Identify transformations to apply\n",
    "rubric={points}\n",
    "\n",
    "Below we are providing possible transformations which can be applied on each column in `census_df`.  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Write whether you agree or not with the proposed transformation and provide your justification or explanation for each row in the _Explanation_ column. An example explanation is given for the `age` feature. \n",
    "\n",
    "> You can find the information about the columns [here](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "> This question is a bit open-ended. If you do not agree with the provided transformation, feel free to argue your case in the explanation. That said, for the autograder to work in the rest of the assignment, go with the transformations provided below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Transformation | Explanation\n",
    "| --- | ----------- | ----- |\n",
    "| age | scaling |  A numeric feature with no missing values. It will be a good idea to apply scaling, as the range of values (17 to 90) is quite different compared to other numeric features.|\n",
    "| workclass | imputation, one-hot encoding | |\n",
    "| fnlwgt | drop |  |\n",
    "| education | ordinal encoding | |\n",
    "| education.num | drop | |\n",
    "| marital.status | one-hot encoding  | |\n",
    "| occupation | imputation, one-hot encoding  | |\n",
    "| relationship | one-hot encoding  | |\n",
    "| race | drop  |  |\n",
    "| sex | one-hot encoding with \"binary=True\" | |\n",
    "| capital.gain | scaling |  | \n",
    "| capital.loss | scaling |  |\n",
    "| hours.per.week | scaling | |\n",
    "| native.country | imputation, one-hot encoding | | \n",
    "\n",
    "\n",
    "_Points:_ 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Identify feature types \n",
    "rubric={autograde}\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "1. Based on the types of transformations we want to apply on the features above, identify different feature types and store them in the variables below as lists.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill in the lists below.\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "ordinal_features = []\n",
    "binary_features = []\n",
    "drop_features = []\n",
    "target = \"income\"\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sorting all the lists above for the autograder\n",
    "numeric_features.sort()\n",
    "categorical_features.sort()\n",
    "ordinal_features.sort()\n",
    "binary_features.sort()\n",
    "drop_features.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Baseline models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Separating feature vectors and targets  \n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create `X_train`, `y_train`, `X_test`, `y_test` from `train_df` and `test_df`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dummy classifier\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out 5-fold cross-validation using `scikit-learn`'s `cross_validate` function with `return_train_scores=True` and store the results as a dataframe named `dummy_df` where each row corresponds to the results from a cross-validation fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_df = None \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3 Discussion\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Hopefully, you were able to run cross-validation with dummy classifier successfully in the question above. At this point, if you train `sklearn`'s `LogisticRegression` model on `X_train` and `y_train` would it work? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Column transformer \n",
    "<hr>\n",
    "\n",
    "In this dataset, we have different types of features: numeric features, an ordinal feature, categorical features, and a binary feature. We want to apply different transformations on different columns and therefore we need a column transformer. First, we'll define different transformations on different types of features and then will create a `scikit-learn`'s `ColumnTransformer` using `make_column_transformer`. For example, the code below creates a `numeric_transformer` for numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercises below, you'll create transformers for other types of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preprocessing ordinal features\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a transformer called `ordinal_transformer` for our ordinal features. \n",
    "\n",
    "> Ordering of some of the education levels is not obvious. Assume that \"HS-grad\" < \"Prof-school\" < \"Assoc-voc\" < \"Assoc-acdm\" < \"Some-college\" < \"Bachelors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_transformer = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Preprocessing binary features\n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a transformer called `binary_transformer` for our binary features.\n",
    "\n",
    "> _Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_transformer = None\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Preprocessing categorical features\n",
    "rubric={autograde}\n",
    "\n",
    "In Exercise 2.3, we saw that there are 3 categorical features with missing values. So first we need to impute the missing values and then encode these features with one-hot encoding. For the purpose of this assignment, let's just have imputation as the first step for all categorical features even when they do not have missing values. This should be OK because if a feature doesn't have any missing value, imputation won't be applied. For imputation you can use [`sklearn`'s SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html). \n",
    "\n",
    "If we want to apply more than one transformation on a set of features, we need to create a [`scikit-learn` `Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). For example, for categorical features we can create a `scikit-learn` `Pipeline` with first step as imputation and the second step as one-hot encoding. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a `sklearn` `Pipeline` using [`make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) called `categorical_transformer` for our categorical features with two steps: `SimpleImputer` for imputation with `strategy=\"constant\"` and `fill_value=\"missing\"` and `OneHotEncoder` with `handle_unknown=\"ignore\"` and `sparse=False` for one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_transformer = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Creating a column transformer. \n",
    "rubric={autograde}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Create a `sklearn` `ColumnTransformer` named `preprocessor` using [`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html) with the transformers defined in the previous exercises. Use the sequence below in the column transformer and add a \"drop\" step for the `drop_features` in the end.  \n",
    "    - `numeric_transformer`\n",
    "    - `ordinal_transformer`\n",
    "    - `binary_transformer`\n",
    "    - `categorical_transformer`\n",
    "2. Transform the data by calling `fit_transform` on the training set and save it as a dataframe in a variable called `transformed_df`. How many new columns have been created in the preprocessed data in comparison to the original `X_train`? Store the difference between the number of columns in `transformed_df` and `X_train` in a variable called `n_new_cols`. \n",
    "\n",
    "> You are not required to do this but optionally you can try to get column names of the transformed data and create the dataframe `transformed_df` with proper column names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_df = None\n",
    "n_new_cols = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Building models \n",
    "\n",
    "Now that we have preprocessed features, we are ready to build models. Below, I'm providing the function we used in class which returns mean cross-validation score along with standard deviation for a given model. Use it to keep track of your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results_dict = {}  # dictionary to store all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I'm showing an example where I call `mean_std_cross_val_scores` with `DummyClassifier`. The function calls `cross_validate` with the passed arguments and returns a series with mean cross-validation results and std of cross-validation. When you train new models, you can just add the results of these models in `results_dict`, which can be easily converted to a dataframe so that you can have a table with all your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(random_state = 123)\n",
    "pipe = make_pipeline(preprocessor, dummy)\n",
    "results_dict[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    pipe, X_train, y_train, cv=5, return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5.1 Trying different classifiers\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. For each of the models in the starter code below: \n",
    "    - Define a pipeline with two steps: `preprocessor` from 4.4 and the model as your classifier. \n",
    "    - Carry out 5-fold cross-validation with the pipeline and get the mean cross-validation scores with std by calling the `mean_std_cross_val_scores` function above. \n",
    "    - Store the results in a dataframe called `income_pred_results_df` with the model names in the `models` dictionary below as the index and each row representing results returned by `mean_std_cross_val_scores` function above. In other words, `income_pred_results_df` should look similar to the `results_df` dataframe above with more rows for the models below. \n",
    "    \n",
    "> This might take a while to run. Be patient! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {\n",
    "    \"decision tree\": DecisionTreeClassifier(random_state=123),\n",
    "    \"kNN\": KNeighborsClassifier(),\n",
    "    \"logistic regression\": LogisticRegression(max_iter=1000, random_state=123),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_pred_results_df = None \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge yourself\n",
    "\n",
    "- Explore [Kaggle's House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/). Identify different feature types and define a column transformer for this dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the assignment! You are now ready to build a simple supervised machine learning pipeline on real-world datasets! Well done :clap:! \n",
    "\n",
    "![](img/eva-well-done.png)\n",
    "\n",
    "Note that this is a simple machine learning pipeline. Here are some of the missing steps: \n",
    "\n",
    "- Feature engineering \n",
    "- Feature selection \n",
    "- carrying out hyperparameter optimization for the most promising models\n",
    "- evaluating performance of the best model on the test set. \n",
    "- interpreting and communicating results"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:ml-workshop]",
   "language": "python",
   "name": "conda-env-ml-workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not train_df is None and not test_df is None, \"Please use the provided variables.\"\n>>> assert train_df.shape == (13024, 15), \"The dimensions of the training set are incorrect\"\n>>> assert test_df.shape == (19537, 15), \"The dimensions of the test set are incorrect\"\n>>> assert train_df.loc[12846][['age', 'education', 'occupation', 'capital.loss']].tolist() == [49, 'Some-college', 'Craft-repair', 0], \"Are you using the provided random state?\"\n>>> assert not 20713 in train_df.index, 'Are you using the provided random state?' \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": [
      1,
      1,
      1,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # Task 1\n>>> assert isinstance(census_summary, pd.DataFrame), \"census_summary dataftame is not created\"\n>>> assert census_summary.shape == (11, 15), \"census_summary shape is incorrect. Probably you are not including all columns\"\n>>> assert census_summary.loc['min']['age'] == 17.0, \"census_summary dataframe is incorrect\"\n>>> assert census_summary.loc['top']['occupation'] == \"Prof-specialty\", \"census_summary dataframe is incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 2\n>>> assert (sha1(str(max_hours_per_week).encode('utf8')).hexdigest() == \"3359de52c8ae993fe0f8fe9c5168a0065bd3c7a4\"), \"max_hours_per_week are incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 3\n>>> assert (sha1(str(most_freq_occupation).encode('utf8')).hexdigest() == \"97165f50eddb0d28a382b0366274e2fe38505644\"), \"most_freq_occupation is incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 4\n>>> assert (sha1(str(missing_vals_cols).encode('utf8')).hexdigest() == \"6bc5e13d4d66b306e52701ee9a1e5e21bf19aeb0\"), \"Please use the exact column/feature name. Also, make sure the lists are sorted.\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 5\n>>> assert (sha1(str(numeric_cols).encode('utf8')).hexdigest() == \"615afaf5011128d641ab8a73289d57bd01a3ec37\"), \"Please use the exact column/feature name. Also, make sure the lists are sorted.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert (sha1(str(numeric_features).encode('utf8')).hexdigest() == \"71401cf60034fd69eee7398866359f612adf3e15\"), \"numeric_features list is not correct\"\n>>> assert (sha1(str(categorical_features).encode('utf8')).hexdigest() == \"af1a4022c0362405678be5c3a6735578a8c0069f\"), \"categorical_features list is not correct\"\n>>> assert (sha1(str(ordinal_features).encode('utf8')).hexdigest() == \"95b86602c44211f3ad662bb58b8e53d024106d05\"), \"ordinal_features list is not correct\"\n>>> assert (sha1(str(binary_features).encode('utf8')).hexdigest() == \"d4b7aa4c56ac2f98e6ac9cec7768484b415b7337\"), \"binary_features list is not correct\"\n>>> assert (sha1(str(drop_features).encode('utf8')).hexdigest() == \"62aab57d42c54be3dfd3c55020e5a167ca1a84c3\"), \"drop_features list is not correct\"\n>>> assert (sha1(str(target).encode('utf8')).hexdigest() == \"0f613350b66e64d92ef21bc4dcdbf8996cb4edf0\"), \"target variable is not set correctly\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not X_train is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not y_train is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not X_test is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not y_test is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert X_train.shape == (13024, 14), \"The dimensions of X_train are incorrect\"\n>>> assert y_train.shape == (13024, ), \"The dimensions of y_train are incorrect. Are you splitting correctly\"\n>>> assert X_test.shape == (19537,14), \"The dimensions of X_test are incorrect. Are you splitting correctly? Are you using single brackets?\"\n>>> assert y_test.shape == (19537,), \"The dimensions of y_test are incorrect. Are you splitting correctly? Are you using single brackets?\"\n>>> assert 'income' not in list(X_train.columns), \"Make sure the target variable is not part of your X dataset.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not dummy_df is None, \"Have you used the correct variable to store the results?\"\n>>> assert sorted(list(dummy_df.columns)) == ['fit_time','score_time','test_score','train_score'], \"Your solution contains incorrect columns.\"\n>>> assert dummy_df.shape == (5,4), \"Are you carrying out 5-fold cross-validation and are you passing return_train_score=True?\"\n>>> assert np.isclose(round(dummy_df['test_score'].mean(),3), 0.758), \"The test scores seem wrong. Are you calling the cross_validate correctly?\"\n>>> assert np.isclose(round(dummy_df['train_score'].mean(),3), 0.758), \"The train scores seem wrong. Are you calling the cross_validate correctly?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.1": {
     "name": "q4.1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not ordinal_transformer is None, \"Are you using the correct variable name?\"\n>>> assert type(ordinal_transformer.get_params()['categories'][0]) is list, \"Are you passing education levels as a list of lists?\"\n>>> assert ordinal_transformer.get_params()['dtype'] == int, \"Please set the dtype to int\"\n>>> assert (sha1(str(ordinal_transformer.get_params()['categories'][0]).encode('utf8')).hexdigest() == \"893a03d114b2af09b53247866c6eea54ebfd090f\") or (sha1(str(ordinal_transformer.get_params()['categories'][0]).encode('utf8')).hexdigest() == \"81059b8bebc9ddb03d61bf07cfd9b9b6b0da288e\"), \"Make sure you are passing categories sorted on levels of education. (Ascending or descending shouldn't matter.)\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.2": {
     "name": "q4.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not binary_transformer is None, \"Are you using the correct variable name?\"\n>>> assert binary_transformer.get_params()['drop'] == 'if_binary', \"Are you passing `drop=if_binary`?\"\n>>> assert binary_transformer.get_params()['dtype'] == int, \"Please set the dtype to int\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.3": {
     "name": "q4.3",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> from sklearn.pipeline import Pipeline\n>>> assert not categorical_transformer is None, \"Are you using the correct variable name?\"\n>>> assert type(categorical_transformer) is Pipeline, \"Are you creating a scikit-learn Pipeline?\"\n>>> assert len(categorical_transformer.get_params()['steps']) == 2, \"Are you creating a pipeline with two steps?\"\n>>> assert categorical_transformer.get_params()['simpleimputer__strategy'] == 'constant', \"Are you passing strategy=constant in the SimpleImputer?\"\n>>> assert categorical_transformer.get_params()['simpleimputer__fill_value'] == 'missing', \"Are you passing fill_value='missing' in the SimpleImputer?\"\n>>> assert categorical_transformer.get_params()['onehotencoder__handle_unknown'] == 'ignore', \"Are you passing handle_unknown = 'ignore' argument to your OHE?\"\n>>> assert categorical_transformer.get_params()['onehotencoder__sparse_output'] == False, \"Are you creating a sparase matrix for OHE?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.4": {
     "name": "q4.4",
     "points": [
      5,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # task 1\n>>> assert not preprocessor is None, \"Are you using the correct variable name?\"\n>>> assert len(preprocessor.get_params()['transformers']) in range(4,6,1), \"Have you included all the transformers?\"\n>>> assert 'onehotencoder' in preprocessor.get_params().keys(), 'Either the categorical_transformer or binary_transformer is not included.'\n>>> assert 'standardscaler' in preprocessor.get_params().keys(), 'numeric_transformer is not included.'\n>>> assert 'ordinalencoder' in preprocessor.get_params().keys(), 'ordinal_transformer is not included.'\n>>> assert 'drop' in preprocessor.get_params().keys(), 'drop features step is not included.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # task 2\n>>> assert not transformed_df is None, \"Are you using the correct variable name?\"\n>>> assert sha1(str(transformed_df.shape).encode('utf8')).hexdigest() == 'a0521f0cdbcd77cd213e7d1a3cfc13c1c7c92a6e', \"The shape of the transformed data is incorrect.\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert sha1(str(n_new_cols).encode('utf8')).hexdigest() == 'b7103ca278a75cad8f7d065acda0c2e80da0b7dc', \"The number of new columns (n_new_cols) is incorrect.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
